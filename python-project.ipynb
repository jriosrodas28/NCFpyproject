{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Cz_avSZICq2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio\n","!pip install matplotlib umap-learn\n","!pip install cupy-cuda11x"],"metadata":{"id":"uYiSt84Mri7S","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# General Imports: Basic Python libraries for system interaction and text processing\n","import os  # For interacting with the operating system (file handling, environment variables)\n","import re  # For working with regular expressions (pattern matching in strings)\n","from collections import Counter  # For counting occurrences of elements (e.g., word frequencies)\n","\n","# Data Processing & Analysis: Libraries for handling and processing structured data\n","import pandas as pd  # Data manipulation and analysis with DataFrames\n","import numpy as np  # Numerical operations, array handling, and matrix math\n","from google.cloud import bigquery  # For interacting with Google BigQuery (cloud data warehouse)\n","\n","# Machine Learning / NLP: Libraries for text analysis, machine learning\n","from sklearn.feature_extraction.text import CountVectorizer  # For converting text into a matrix of token counts (Bag of Words)\n","from sklearn.metrics.pairwise import cosine_similarity  # For measuring similarity between vectors (cosine similarity)\n","import torch  # Deep learning framework (PyTorch), for building and training neural networks\n","from sentence_transformers import SentenceTransformer  # For sentence embeddings using pre-trained transformer models\n","\n","# Interface / Web: Libraries for building interactive web interfaces for machine learning models\n","import gradio as gr  # For creating interactive applications and demos for machine learning models\n","\n","# Progress Bar: Library to display a progress bar for loops or long-running tasks\n","from tqdm import tqdm  # For creating a progress bar in loops (visualizing the progress of time-consuming tasks)\n","\n","# Data Visualization & Dimensionality Reduction: Libraries for visualizing and reducing the dimensionality of data\n","import seaborn as sns  # For creating attractive and informative statistical graphics\n","from matplotlib.lines import Line2D  # For custom line creation in plots\n","import matplotlib.pyplot as plt  # For creating static, animated, and interactive plots\n","from sklearn.manifold import TSNE  # For t-SNE dimensionality reduction technique\n","import umap  # For Uniform Manifold Approximation and Projection (UMAP)\n","import cupy as cp  # For GPU-accelerated computation (NumPy-like operations with CUDA support)\n","\n","# Image Processing: Libraries for handling image data in memory and processing images\n","from io import BytesIO  # For handling image byte streams\n","from PIL import Image  # For image manipulation and processing\n","\n"],"metadata":{"id":"Pugij5yroY4W","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Part 1\n"],"metadata":{"id":"6Kh1OoQHdA6R"}},{"cell_type":"markdown","source":["functions"],"metadata":{"id":"sDis2bEGSznB"}},{"cell_type":"code","source":["def clean_string(text):\n","    '''clean the string\n","    '''\n","    # Remove punctuation and special characters (keep only letters and spaces)\n","    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Normalize spaces (replace multiple spaces with a single space)\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n","\n","    # Strip leading and trailing spaces\n","    cleaned_text = cleaned_text.strip()\n","\n","    # Convert to lowercase\n","    cleaned_text = cleaned_text.lower()\n","\n","    return cleaned_text\n","\n","def clean_n_org(query_job):\n","    '''organize the query into dataframe and appyle clean_string\n","       to Context column\n","    '''\n","    # Get query result as DataFrame\n","    data = query_job.result().to_dataframe()\n","\n","    data['Context'] = data['Context'].fillna('').astype(str)\n","\n","    # Convert 'DateTime' column to datetime and extract date part\n","    data['DateTime'] = pd.to_datetime(data['DateTime'], errors='raise')\n","    data['DateTime'] = data['DateTime'].dt.date\n","\n","    # Remove duplicates based on 'Context' column\n","    data.drop_duplicates(subset=['Context'], inplace=True)\n","\n","    # Clean the 'Context' column\n","    data['Context'] = data['Context'].apply(clean_string)\n","    idx = data['Context'] == ''\n","    data = data.drop(data[idx].index)\n","\n","    # Return the cleaned data\n","    monthly_data = data\n","    return monthly_data\n","\n","def word_dist_bytopic(df):\n","    ''' create a count of the words by topic\n","    '''\n","    #Group by topic and context again making sure column type is set\n","\n","    grouped = df.groupby('Topic')['Context'].apply(lambda x: ' '.join(x.astype(str))).reset_index()\n","\n","    #tokenizes the text and counts the frequency of each word, ignoring words like \"is\", \"the\", \"and\", etc\n","    vectorizer = CountVectorizer(stop_words='english')\n","\n","    word_distributions = {}\n","\n","    for index, row in grouped.iterrows():\n","        topic = row['Topic']\n","        text = row['Context']\n","\n","        # Vectorizing the text and getting word counts\n","        word_counts = vectorizer.fit_transform([text]).toarray().sum(axis=0)\n","\n","        # Get the words corresponding to the vectorized text\n","        words = vectorizer.get_feature_names_out()\n","\n","        # Map words to their counts and store in word_distributions\n","        word_distributions[topic] = dict(zip(words, word_counts))\n","\n","    return word_distributions\n","\n","def top_words_bytopic(word_distributions, num):\n","  '''Prints the top `num` most frequent words for each topic.\n","  '''\n","  for topic, word_count in word_distributions.items():\n","    print(f\"Topic: {topic}\")\n","\n","    # print the top `num` most common words for the topic using Counter\n","    print(Counter(word_count).most_common(num))\n","\n","    print('\\n')\n","\n","def top_words_bytopic(word_distributions, num):\n","    '''Prints the top `num` most frequent words for each topic and returns a DataFrame.'''\n","\n","    top_words = []\n","\n","    # Loop through each topic and its associated word counts\n","    for topic, word_count in word_distributions.items():\n","        print(f\"Topic: {topic}\")\n","\n","        # Get the top `num` most common words for the topic\n","        most_common_words = Counter(word_count).most_common(num)\n","\n","        # Print the top words for this topic\n","        for word, count in most_common_words:\n","            print(f\"{word}: {count}\")\n","\n","        # Add the results to the list for DataFrame\n","        for word, count in most_common_words:\n","            top_words.append({\n","                'Topic': topic,\n","                'Word': word,\n","                'Frequency': count\n","            })\n","\n","        print('\\n')\n","\n","    # Convert the results to a DataFrame\n","    top_words_df = pd.DataFrame(top_words)\n","\n","    return top_words_df"],"metadata":{"id":"olnh3CKWS29x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Call the API"],"metadata":{"id":"ignIee1_2joa"}},{"cell_type":"code","source":["os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/content/drive/MyDrive/py_project/BigQuerykeyfile.json\"\n","\n","client = bigquery.Client()"],"metadata":{"id":"XXoCtlk6SaBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sample query and result"],"metadata":{"id":"F-awg0M82n87"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NaGhRPpxt5Qy"},"outputs":[],"source":["\n","#gdelt_query =\n","\"\"\"\n"," SELECT Topic, COUNT(*)\n"," FROM `gdelt-bq.covid19.onlinenews`\n"," WHERE TIMESTAMP_TRUNC(DateTime, DAY) BETWEEN TIMESTAMP(\"2020-02-01\") AND TIMESTAMP(\"2020-02-02\")\n"," Group BY Topic\n"," \"\"\"\n","\n","# query_job = client.query(gdelt_query)\n","\n","# print(query_job)\n","# for row in query_job.result():\n","#   print(row[0], row[1])\n","\n","#output\n","'''\n","Falsehoods 912\n","Cases 11258\n","Masks 4570\n","Panic 916\n","Quarantine 8259\n","Testing 6293\n","Covid19 20738\n","Prices 624\n","Shortages 196\n","Ventilators 13\n","SocialDistancing 2\n","'''"]},{"cell_type":"code","source":["# x = sum(df['f0_'])\n","# df = query_job.result().to_dataframe()\n","# print(df, x)"],"metadata":{"id":"FwepDT10YuWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Query GDELT covid19 table for monthly data"],"metadata":{"id":"tssp6e_m3XPt"}},{"cell_type":"code","source":["#2020 Month Collected as_of_today[feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec]\n","#2021 Month Collected as_of_today[jan]\n","\n","testq = '''\n","SELECT *\n","FROM `gdelt-bq.covid19.onlinenews`\n","WHERE TIMESTAMP_TRUNC(DateTime, DAY) BETWEEN TIMESTAMP(\"2021-01-01\") AND TIMESTAMP(\"2021-01-31\")\n","    AND REGEXP_CONTAINS(LOWER(Context), r'vaccine')\n","'''\n","query_job = client.query(testq)\n"],"metadata":{"id":"UiqYQ40fxo08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_job.result()"],"metadata":{"id":"xNRjGafWEoXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clean and organize the monthly query then\n","Collect output and store as CSV file"],"metadata":{"id":"Ja9JFvhM4IPl"}},{"cell_type":"code","source":["output = clean_n_org(query_job).dropna()\n","output.to_csv('jan_data.csv', index=False)  # Don't save the index by default"],"metadata":{"id":"TnZqxXBMZWNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload CSV file"],"metadata":{"id":"QmbZZwzn40Fw"}},{"cell_type":"code","source":["upload = pd.read_csv('jan_data.csv').dropna() # Don't save the index by default\n"],"metadata":{"id":"LUrF2JjEeqt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find word distribution by topic and list top most common words"],"metadata":{"id":"zgkwtJ65yeYL"}},{"cell_type":"code","source":["word_freq = word_dist_bytopic(upload)"],"metadata":{"id":"BNBVUHVd7BIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_count = top_words_bytopic(word_freq, 10)"],"metadata":{"id":"fjWyf0qVWU9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Part 2\n"],"metadata":{"id":"Qs_gfT2MdFgN"}},{"cell_type":"markdown","source":["Collect monthly data from 2020-02 : 2021-01 and combine to one df"],"metadata":{"id":"CCVY52YRgv8w"}},{"cell_type":"code","source":["'''\n","import glob\n","\n","# Step 1: Use glob to find all CSV files in the directory\n","csv_files = glob.glob('/content/*.csv')\n","\n","# Step 2: Read each CSV file into a list of DataFrames\n","dfs = [pd.read_csv(file) for file in csv_files]\n","\n","# Step 3: Concatenate all DataFrames into a single DataFrame\n","combined_df = pd.concat(dfs, ignore_index=True)\n","\n","# Step 4: Drop Date column\n","combined_df = combined_df.drop(columns=['DateTime'])\n","\n","# Optionally, you can reset the index after concatenation:\n","combined_df.reset_index(drop=True, inplace=True)\n","'''"],"metadata":{"id":"SVdEe-DkdNV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Collect full dataset and output CSV"],"metadata":{"id":"F1JxjqR8wZqp"}},{"cell_type":"code","source":["'''\n","combo_output = combined_df.dropna()\n","combo_output.to_csv('one_year_data.csv', index=False)\n","'''"],"metadata":{"id":"vjHowEd8fkD4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload full dataset CSV file"],"metadata":{"id":"dX-u6aAfw7mC"}},{"cell_type":"code","source":["combo_upload = pd.read_csv('/content/drive/MyDrive/py_project/one_year_data.csv').dropna()"],"metadata":{"id":"wFiUZRggw66w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find word distribution by topic and list top most common words"],"metadata":{"id":"HD-yvQn-20Nl"}},{"cell_type":"code","source":["full_word_freq = word_dist_bytopic(combo_upload)"],"metadata":{"id":"TvaDffDdiooP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_s7GtyY9spTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_top_count = top_words_bytopic(full_word_freq, 10)"],"metadata":{"id":"IZSd5rXgixpe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The context and topic columns are populated based on the textual analysis of the article's content. GDELT uses natural language processing (NLP) algorithms to examine the text of the article, identifying key themes, entities, and relationships between them. The context refers to the broader or underlying themes of the article, and these are typically categorized into specific topics related to COVID-19."],"metadata":{"id":"A-hZy5o6EqF_"}},{"cell_type":"markdown","source":["Filter the yearly data by keywords"],"metadata":{"id":"yAj9t4iWorRn"}},{"cell_type":"code","source":["vaccine_effectiveness_keywords = [\n","    \"efficacy\",\n","    \"effectiveness\",\n","    \"vaccine effectiveness\",\n","    \"vaccine efficacy\",\n","    \"protection\",\n","    \"protective\",\n","    \"protection level\",\n","    \"protection rate\",\n","    \"protection efficacy\",\n","    \"vaccine response\",\n","    \"booster effect\",\n","    \"prevention\",\n","    \"immunity\",\n","    \"long-term immunity\",\n","    \"population immunity\",\n","    \"herd immunity\",\n","    \"viral load reduction\",\n","    \"clinical trials\",\n","    \"trial results\",\n","    \"antibody response\",\n","    \"immune response\",\n","    \"cross-protection\",\n","    \"breakthrough cases\",\n","    \"vaccine failure rate\",\n","    'pfizer',\n","    'mRNA',\n","    'moderna',\n","    'novavax',\n","    'johnson'\n","]\n","\n","l = [\"efficacy\", 'effectiveness', 'immunity']\n","\n","#Creates a regular expression pattern to match any of the words in the given list `l`. The pattern matches whole words, ensuring that matches occur only at word boundaries.\n","pattern = r'\\b(?:' + '|'.join(map(re.escape, l)) + r')\\b'\n","\n","# Filter rows that contain any word from the list\n","filtered_df = combo_upload[combo_upload['Context'].str.contains(pattern, regex=True, na=False)]\n","\n","#filtered DataFrame to csv\n","#filtered_df.to_csv('filtered_data.csv', index=False)"],"metadata":{"id":"EACeofxmSXol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load filtered csv"],"metadata":{"id":"EjKM9umwo2Kn"}},{"cell_type":"code","source":["filtered_df  = pd.read_csv('/content/drive/MyDrive/py_project/filtered_data.csv').dropna()\n"],"metadata":{"id":"y4jcuoehP6mS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find the count of words in the Context colum and find mean"],"metadata":{"id":"3TTJtXKro-cm"}},{"cell_type":"code","source":["# Count the number of words in each row of the 'Context' column\n","filtered_df['word_count'] = filtered_df['Context'].apply(lambda x: len(str(x).split()))\n","\n","# Calculate the mean number of words across all rows in the 'Context' column\n","mean_word_count = filtered_df['word_count'].mean()\n","\n","# Display the word counts and mean\n","print(filtered_df[['Context', 'word_count']])\n","print(f\"Mean number of words: {mean_word_count}\")\n"],"metadata":{"id":"N6hzo8CJ92h7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preparing to create embedding for Context column"],"metadata":{"id":"y5rQxX_LpJKW"}},{"cell_type":"markdown","source":["Load embedding model"],"metadata":{"id":"SUZRS1S7pXjH"}},{"cell_type":"code","source":["# Load a pre-trained model\n","model = SentenceTransformer('all-MiniLM-L6-v2')"],"metadata":{"id":"P0JYnWWBpT1P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given size of data embeddings are done in batches,\n","and Processing is done using google T4-GPU"],"metadata":{"id":"K1FpRThEp2Wo"}},{"cell_type":"code","source":["\n","print(torch.cuda.is_available())  # Should return True if a GPU is available"],"metadata":{"id":"QgpxJEoWUKtQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DO ONCE\n","\n","batch_size = 64  # Adjust the batch size based on GPU memory\n","embeddings = []\n","\n","for i in tqdm(range(0, len(filtered_df), batch_size)):\n","    batch = filtered_df['Context'].iloc[i:i + batch_size]\n","    embeddings_batch = model.encode(batch.tolist(), batch_size=batch_size, show_progress_bar=True, device='cuda')\n","    embeddings.extend(embeddings_batch)\n","\n","# Now, `embeddings` holds the result for all the sentences"],"metadata":{"collapsed":true,"id":"iM6zN8UFBRKF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add embedding to filtered_data"],"metadata":{"id":"XhqMuLVTqRVI"}},{"cell_type":"code","source":["filtered_df['embeddings'] = embeddings\n","#write file to drive\n","#filtered_df.to_csv('/content/drive/MyDrive/py_project/embedded_filtered_data', index=False)"],"metadata":{"id":"bg3yWmd2ZGDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load embedded_data"],"metadata":{"id":"jByJh925qWgl"}},{"cell_type":"code","source":["embedded_data  = pd.read_csv('/content/drive/MyDrive/py_project/embedded_filtered_data.csv').dropna()\n","#format embeddings to correct type since I saved as csv\n","embedded_data['embeddings'] = embedded_data['embeddings'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))"],"metadata":{"collapsed":true,"id":"fxlAZdiWdfNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedded_data"],"metadata":{"id":"7wK9MayIMzar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function the searches context via embedded query"],"metadata":{"id":"4REMEnX_qgII"}},{"cell_type":"code","source":["def semantic_search(query, top_k=100):\n","    # Load the model\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","    # Generate embedding for the query\n","    query_embedding = model.encode(query)\n","\n","    # Compute cosine similarity between query and all documents\n","    similarities = cosine_similarity([query_embedding], embedded_data['embeddings'].tolist())\n","\n","    # Get indices of the top K most similar documents\n","    top_k_indices = similarities[0].argsort()[-top_k:][::-1]\n","\n","    # Prepare the results as a list of dictionaries, including Topic, URL, Context, and Similarity Score\n","    plt_index = []\n","    matches = []\n","    for i in top_k_indices:\n","        topic = embedded_data['Topic'].iloc[i]  # Assuming 'Topic' column exists\n","        url = embedded_data['URL'].iloc[i]      # Assuming 'URL' column exists\n","        context = embedded_data['Context'].iloc[i]  # Assuming 'Context' column exists\n","        similarity_score = similarities[0][i]\n","        index = i\n","\n","        plt_index.append(int(index))\n","\n","        matches.append({\n","\n","            \"Topic\": topic,\n","            \"URL\": url,\n","            \"Context\": context,\n","            \"Similarity Score\": similarity_score\n","        })\n","\n","    return matches, plt_index"],"metadata":{"id":"DRT-JbY3dvVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","x = [i for i in range(0,len(embedded_data))]"],"metadata":{"id":"XORo-LLSFH_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["semantic_search(\"academics talking about vaccine\")"],"metadata":{"id":"RZxlA3fjfy6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#visualizing context by topics"],"metadata":{"id":"Yvi-800XX6QB"}},{"cell_type":"code","source":["#run once\n","umap_model = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=42)  # UMAP for 2D visualization\n","reduced_embeddings = umap_model.fit_transform(embedded_data['embeddings'].tolist())\n","\n"],"metadata":{"id":"t-9spdcpYEzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create dataFrame from reduced embeddings"],"metadata":{"id":"i8vtO5Y8pW_T"}},{"cell_type":"code","source":["reduced_df = pd.DataFrame(reduced_embeddings, columns=['UMAP_Component_1', 'UMAP_Component_2'])\n","reduced_df['Topic'] = embedded_data['Topic']\n","reduced_df['URL'] = embedded_data['URL']\n","reduced_df['Context'] = embedded_data['Context']\n","reduced_df\n","reduced_df.to_csv('/content/drive/MyDrive/py_project/reduced_embedding_data.csv', index=False)"],"metadata":{"id":"RfpX2Qtmf7uk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read reduced_embedding_data"],"metadata":{"id":"BbZ9khJdphma"}},{"cell_type":"code","source":["reduced_df = pd.read_csv('/content/drive/MyDrive/py_project/reduced_embedding_data.csv')\n"],"metadata":{"id":"BcLDBwoxkLWd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the cluster of topics"],"metadata":{"id":"SMHghHakpwLb"}},{"cell_type":"code","source":["def plot_topics_for_indexes(indexes):\n","    # Filter the DataFrame to include only the rows specified by the indexes\n","    filtered_df = reduced_df.iloc[indexes]\n","\n","    # Get unique topics from the filtered DataFrame\n","    unique_topics = filtered_df['Topic'].unique()\n","\n","    # Create a color palette for each unique topic\n","    palette = sns.color_palette(\"Set2\", len(unique_topics))  # \"Set2\" is just an example, feel free to change\n","\n","    # Map each topic to a unique color\n","    topic_color_map = {topic: palette[i] for i, topic in enumerate(unique_topics)}\n","\n","    # Create the scatter plot with colors based on the 'Topic' column\n","    plt.figure(figsize=(10, 8))\n","\n","    # Plot each category separately with corresponding color\n","    for topic, color in topic_color_map.items():\n","        topic_data = filtered_df[filtered_df['Topic'] == topic]\n","        plt.scatter(\n","            topic_data['UMAP_Component_1'],\n","            topic_data['UMAP_Component_2'],\n","            color=color,  # Assign the correct color based on the topic\n","            label=topic,  # Use the topic name as label for the legend\n","            alpha=0.6\n","        )\n","\n","    # Add the legend to the plot\n","    plt.legend(labels=unique_topics, title=\"Topic\")\n","\n","    # Customize the plot's title and axis labels\n","    plt.title(\"Visualization of Embeddings\")\n","    plt.xlabel(\"UMAP Component 1\")\n","    plt.ylabel(\"UMAP Component 2\")\n","\n","    # Save the plot to a BytesIO buffer\n","    buf = BytesIO()\n","    plt.savefig(buf, format='png')\n","    buf.seek(0)  # Rewind the buffer to the beginning\n","    plt.close()  # Close the plot to avoid memory issues\n","\n","    # Return the image as a PIL object\n","    pil_image = Image.open(buf)\n","    return pil_image\n","\n","#save_path = '/content/drive/MyDrive/py_project/umap_plot.png'\n","#plt.savefig(save_path)"],"metadata":{"id":"j4mOzNRcZCVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_topics_for_indexes(x)"],"metadata":{"id":"KzcrBSw6Fana"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Search interface\n"],"metadata":{"id":"9PrcJTiX93I6"}},{"cell_type":"markdown","source":["interface function to interact with search function"],"metadata":{"id":"VXe7ssRirGYm"}},{"cell_type":"code","source":["def gradio_interface(query):\n","    matches, plt_index = semantic_search(query)\n","\n","    result_str = \"\"\n","    for rec in matches:\n","        result_str += f\"<strong>Topic:</strong> {rec['Topic']}<br>\"\n","        result_str += f\"<strong>URL:</strong> <a href='{rec['URL']}' target='_blank'>{rec['URL']}</a><br>\"\n","        result_str += f\"<strong>Context:</strong> {rec['Context']}<br>\"\n","        result_str += f\"<strong>Similarity Score:</strong> {rec['Similarity Score']:.4f}<br>\"\n","        result_str += \"<hr>\"  # Horizontal rule for separation between entries\n","\n","    plot_image = plot_topics_for_indexes(plt_index)\n","\n","    return plot_image, result_str\n"],"metadata":{"id":"yFQmaJ7mrdf3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"yEHXifB7rVM1"}},{"cell_type":"code","source":["gradio_interface('academics talking about vaccine')"],"metadata":{"id":"duRySt7AuBWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define and launch the inteface for use"],"metadata":{"id":"gxLCBLjsrNnM"}},{"cell_type":"code","source":["# Define the Gradio interface\n","iface = gr.Interface(\n","    fn=gradio_interface,                     # Function to run on input\n","    inputs=gr.Textbox(label=\"Enter search text\"),  # Input: Textbox for query text\n","    outputs=[gr.Image(label=\"Plot Image\"), gr.HTML(label=\"Related Articles\")] # Output: Textbox for recommendations\n",")\n","\n","# Launch the Gradio interface\n","iface.launch()"],"metadata":{"id":"8HtfVwFJtfjh"},"execution_count":null,"outputs":[]}]}